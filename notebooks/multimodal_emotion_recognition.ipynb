{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73UV3HE4EVPu",
        "outputId": "d43a0428-53ee-4f05-91d6-359680261bcd"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/Multimodal emotion recognition\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_9awsfBQkOC",
        "outputId": "073ca63d-8a71-4e6a-d0ef-f2bbcc210cee"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Multimodal emotion recognition\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py\n",
        "# if everything went good then u'll get :\n",
        "# load the dataset , start preprocessing , begin training and save outputs in checkpoints/ and results/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWXpKu4dRkhy",
        "outputId": "10f99acb-ef5b-497b-b2cc-ae9dc4461f36"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-02 14:48:51.115358: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762094931.134760   38960 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762094931.140671   38960 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762094931.155598   38960 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762094931.155624   38960 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762094931.155628   38960 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762094931.155632   38960 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-02 14:48:51.160059: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Using device: cuda\n",
            "Loading dataset...\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Train samples: 9989\n",
            "Val samples: 1109\n",
            "Test samples: 2610\n",
            "\n",
            "Initializing CV-Grade Architecture...\n",
            "\n",
            "Starting training...\n",
            "Epoch 0, Batch 10/313, Loss: 1.9066\n",
            "Epoch 0, Batch 20/313, Loss: 1.7794\n",
            "Epoch 0, Batch 30/313, Loss: 1.7247\n",
            "Epoch 0, Batch 40/313, Loss: 1.6944\n",
            "Epoch 0, Batch 50/313, Loss: 1.6796\n",
            "Epoch 0, Batch 60/313, Loss: 1.7441\n",
            "Epoch 0, Batch 70/313, Loss: 1.4541\n",
            "Epoch 0, Batch 80/313, Loss: 1.5628\n",
            "Epoch 0, Batch 90/313, Loss: 1.4273\n",
            "Epoch 0, Batch 100/313, Loss: 1.4098\n",
            "Epoch 0, Batch 110/313, Loss: 1.4241\n",
            "Epoch 0, Batch 120/313, Loss: 1.2539\n",
            "Epoch 0, Batch 130/313, Loss: 1.2174\n",
            "Epoch 0, Batch 140/313, Loss: 1.3647\n",
            "Epoch 0, Batch 150/313, Loss: 1.4921\n",
            "Epoch 0, Batch 160/313, Loss: 1.2894\n",
            "Epoch 0, Batch 170/313, Loss: 1.3063\n",
            "Epoch 0, Batch 180/313, Loss: 1.5386\n",
            "Epoch 0, Batch 190/313, Loss: 1.4085\n",
            "Epoch 0, Batch 200/313, Loss: 1.2489\n",
            "Epoch 0, Batch 210/313, Loss: 1.4154\n",
            "Epoch 0, Batch 220/313, Loss: 1.4631\n",
            "Epoch 0, Batch 230/313, Loss: 1.2334\n",
            "Epoch 0, Batch 240/313, Loss: 1.2488\n",
            "Epoch 0, Batch 250/313, Loss: 1.3427\n",
            "Epoch 0, Batch 260/313, Loss: 1.2918\n",
            "Epoch 0, Batch 270/313, Loss: 1.6666\n",
            "Epoch 0, Batch 280/313, Loss: 1.0497\n",
            "Epoch 0, Batch 290/313, Loss: 1.0529\n",
            "Epoch 0, Batch 300/313, Loss: 1.2699\n",
            "Epoch 0, Batch 310/313, Loss: 1.3394\n",
            "Epoch 1/15\n",
            "  Train Loss: 1.4308, Train Acc: 0.5253\n",
            "  Val Loss: 1.3241, Val Acc: 0.5582, Val F1: 0.4822\n",
            "  Best model saved!\n",
            "Epoch 1, Batch 10/313, Loss: 1.1934\n",
            "Epoch 1, Batch 20/313, Loss: 1.4296\n",
            "Epoch 1, Batch 30/313, Loss: 1.1283\n",
            "Epoch 1, Batch 40/313, Loss: 1.5788\n",
            "Epoch 1, Batch 50/313, Loss: 1.3921\n",
            "Epoch 1, Batch 60/313, Loss: 1.0466\n",
            "Epoch 1, Batch 70/313, Loss: 1.1542\n",
            "Epoch 1, Batch 80/313, Loss: 1.3025\n",
            "Epoch 1, Batch 90/313, Loss: 1.5992\n",
            "Epoch 1, Batch 100/313, Loss: 1.2311\n",
            "Epoch 1, Batch 110/313, Loss: 1.4216\n",
            "Epoch 1, Batch 120/313, Loss: 1.2241\n",
            "Epoch 1, Batch 130/313, Loss: 1.2629\n",
            "Epoch 1, Batch 140/313, Loss: 1.0294\n",
            "Epoch 1, Batch 150/313, Loss: 1.2953\n",
            "Epoch 1, Batch 160/313, Loss: 1.2656\n",
            "Epoch 1, Batch 170/313, Loss: 1.2545\n",
            "Epoch 1, Batch 180/313, Loss: 1.2022\n",
            "Epoch 1, Batch 190/313, Loss: 1.2303\n",
            "Epoch 1, Batch 200/313, Loss: 1.4198\n",
            "Epoch 1, Batch 210/313, Loss: 1.3274\n",
            "Epoch 1, Batch 220/313, Loss: 1.1112\n",
            "Epoch 1, Batch 230/313, Loss: 1.0675\n",
            "Epoch 1, Batch 240/313, Loss: 1.0797\n",
            "Epoch 1, Batch 250/313, Loss: 1.0772\n",
            "Epoch 1, Batch 260/313, Loss: 1.2388\n",
            "Epoch 1, Batch 270/313, Loss: 1.1620\n",
            "Epoch 1, Batch 280/313, Loss: 1.6390\n",
            "Epoch 1, Batch 290/313, Loss: 1.0092\n",
            "Epoch 1, Batch 300/313, Loss: 1.1328\n",
            "Epoch 1, Batch 310/313, Loss: 1.5114\n",
            "Epoch 2/15\n",
            "  Train Loss: 1.2220, Train Acc: 0.5903\n",
            "  Val Loss: 1.2754, Val Acc: 0.5681, Val F1: 0.5064\n",
            "  Best model saved!\n",
            "Epoch 2, Batch 10/313, Loss: 0.8317\n",
            "Epoch 2, Batch 20/313, Loss: 0.9996\n",
            "Epoch 2, Batch 30/313, Loss: 1.0015\n",
            "Epoch 2, Batch 40/313, Loss: 0.9102\n",
            "Epoch 2, Batch 50/313, Loss: 1.1165\n",
            "Epoch 2, Batch 60/313, Loss: 1.1296\n",
            "Epoch 2, Batch 70/313, Loss: 0.9288\n",
            "Epoch 2, Batch 80/313, Loss: 1.2595\n",
            "Epoch 2, Batch 90/313, Loss: 1.5107\n",
            "Epoch 2, Batch 100/313, Loss: 1.1478\n",
            "Epoch 2, Batch 110/313, Loss: 1.5836\n",
            "Epoch 2, Batch 120/313, Loss: 1.0698\n",
            "Epoch 2, Batch 130/313, Loss: 1.0766\n",
            "Epoch 2, Batch 140/313, Loss: 0.8793\n",
            "Epoch 2, Batch 150/313, Loss: 1.2658\n",
            "Epoch 2, Batch 160/313, Loss: 1.2318\n",
            "Epoch 2, Batch 170/313, Loss: 1.0462\n",
            "Epoch 2, Batch 180/313, Loss: 0.9515\n",
            "Epoch 2, Batch 190/313, Loss: 0.9985\n",
            "Epoch 2, Batch 200/313, Loss: 1.1803\n",
            "Epoch 2, Batch 210/313, Loss: 0.9258\n",
            "Epoch 2, Batch 220/313, Loss: 1.3466\n",
            "Epoch 2, Batch 230/313, Loss: 1.1402\n",
            "Epoch 2, Batch 240/313, Loss: 1.2482\n",
            "Epoch 2, Batch 250/313, Loss: 1.2589\n",
            "Epoch 2, Batch 260/313, Loss: 1.4499\n",
            "Epoch 2, Batch 270/313, Loss: 1.1200\n",
            "Epoch 2, Batch 280/313, Loss: 1.3170\n",
            "Epoch 2, Batch 290/313, Loss: 0.9255\n",
            "Epoch 2, Batch 300/313, Loss: 1.3269\n",
            "Epoch 2, Batch 310/313, Loss: 1.1768\n",
            "Epoch 3/15\n",
            "  Train Loss: 1.1829, Train Acc: 0.6053\n",
            "  Val Loss: 1.2555, Val Acc: 0.5843, Val F1: 0.5295\n",
            "  Best model saved!\n",
            "Epoch 3, Batch 10/313, Loss: 1.3549\n",
            "Epoch 3, Batch 20/313, Loss: 1.2219\n",
            "Epoch 3, Batch 30/313, Loss: 0.8307\n",
            "Epoch 3, Batch 40/313, Loss: 0.9897\n",
            "Epoch 3, Batch 50/313, Loss: 1.0189\n",
            "Epoch 3, Batch 60/313, Loss: 1.4113\n",
            "Epoch 3, Batch 70/313, Loss: 0.9188\n",
            "Epoch 3, Batch 80/313, Loss: 1.3481\n",
            "Epoch 3, Batch 90/313, Loss: 0.9148\n",
            "Epoch 3, Batch 100/313, Loss: 1.0557\n",
            "Epoch 3, Batch 110/313, Loss: 1.1245\n",
            "Epoch 3, Batch 120/313, Loss: 1.1254\n",
            "Epoch 3, Batch 130/313, Loss: 1.0751\n",
            "Epoch 3, Batch 140/313, Loss: 1.2619\n",
            "Epoch 3, Batch 150/313, Loss: 0.9846\n",
            "Epoch 3, Batch 160/313, Loss: 1.1888\n",
            "Epoch 3, Batch 170/313, Loss: 1.2426\n",
            "Epoch 3, Batch 180/313, Loss: 1.0189\n",
            "Epoch 3, Batch 190/313, Loss: 0.9976\n",
            "Epoch 3, Batch 200/313, Loss: 0.9349\n",
            "Epoch 3, Batch 210/313, Loss: 1.1437\n",
            "Epoch 3, Batch 220/313, Loss: 1.4525\n",
            "Epoch 3, Batch 230/313, Loss: 1.1096\n",
            "Epoch 3, Batch 240/313, Loss: 0.8634\n",
            "Epoch 3, Batch 250/313, Loss: 1.2255\n",
            "Epoch 3, Batch 260/313, Loss: 1.1152\n",
            "Epoch 3, Batch 270/313, Loss: 1.2723\n",
            "Epoch 3, Batch 280/313, Loss: 1.4426\n",
            "Epoch 3, Batch 290/313, Loss: 1.3683\n",
            "Epoch 3, Batch 300/313, Loss: 0.9824\n",
            "Epoch 3, Batch 310/313, Loss: 0.9813\n",
            "Epoch 4/15\n",
            "  Train Loss: 1.1530, Train Acc: 0.6135\n",
            "  Val Loss: 1.2470, Val Acc: 0.5834, Val F1: 0.5373\n",
            "Epoch 4, Batch 10/313, Loss: 1.1827\n",
            "Epoch 4, Batch 20/313, Loss: 1.0610\n",
            "Epoch 4, Batch 30/313, Loss: 1.5790\n",
            "Epoch 4, Batch 40/313, Loss: 1.2126\n",
            "Epoch 4, Batch 50/313, Loss: 1.0135\n",
            "Epoch 4, Batch 60/313, Loss: 0.9441\n",
            "Epoch 4, Batch 70/313, Loss: 1.0025\n",
            "Epoch 4, Batch 80/313, Loss: 0.9855\n",
            "Epoch 4, Batch 90/313, Loss: 1.5614\n",
            "Epoch 4, Batch 100/313, Loss: 1.2647\n",
            "Epoch 4, Batch 110/313, Loss: 0.8131\n",
            "Epoch 4, Batch 120/313, Loss: 1.1199\n",
            "Epoch 4, Batch 130/313, Loss: 0.7918\n",
            "Epoch 4, Batch 140/313, Loss: 1.1825\n",
            "Epoch 4, Batch 150/313, Loss: 0.8558\n",
            "Epoch 4, Batch 160/313, Loss: 1.1179\n",
            "Epoch 4, Batch 170/313, Loss: 1.1183\n",
            "Epoch 4, Batch 180/313, Loss: 0.8626\n",
            "Epoch 4, Batch 190/313, Loss: 1.4388\n",
            "Epoch 4, Batch 200/313, Loss: 1.1788\n",
            "Epoch 4, Batch 210/313, Loss: 0.7355\n",
            "Epoch 4, Batch 220/313, Loss: 1.1189\n",
            "Epoch 4, Batch 230/313, Loss: 0.9911\n",
            "Epoch 4, Batch 240/313, Loss: 1.3979\n",
            "Epoch 4, Batch 250/313, Loss: 1.2695\n",
            "Epoch 4, Batch 260/313, Loss: 1.0263\n",
            "Epoch 4, Batch 270/313, Loss: 1.0914\n",
            "Epoch 4, Batch 280/313, Loss: 0.9243\n",
            "Epoch 4, Batch 290/313, Loss: 0.9929\n",
            "Epoch 4, Batch 300/313, Loss: 1.6993\n",
            "Epoch 4, Batch 310/313, Loss: 1.4698\n",
            "Epoch 5/15\n",
            "  Train Loss: 1.1359, Train Acc: 0.6194\n",
            "  Val Loss: 1.2372, Val Acc: 0.5870, Val F1: 0.5376\n",
            "  Best model saved!\n",
            "Epoch 5, Batch 10/313, Loss: 1.0417\n",
            "Epoch 5, Batch 20/313, Loss: 1.3675\n",
            "Epoch 5, Batch 30/313, Loss: 1.1740\n",
            "Epoch 5, Batch 40/313, Loss: 1.1953\n",
            "Epoch 5, Batch 50/313, Loss: 0.8580\n",
            "Epoch 5, Batch 60/313, Loss: 1.3050\n",
            "Epoch 5, Batch 70/313, Loss: 1.0866\n",
            "Epoch 5, Batch 80/313, Loss: 1.1320\n",
            "Epoch 5, Batch 90/313, Loss: 1.3574\n",
            "Epoch 5, Batch 100/313, Loss: 1.1553\n",
            "Epoch 5, Batch 110/313, Loss: 1.0209\n",
            "Epoch 5, Batch 120/313, Loss: 0.9750\n",
            "Epoch 5, Batch 130/313, Loss: 1.3795\n",
            "Epoch 5, Batch 140/313, Loss: 1.2531\n",
            "Epoch 5, Batch 150/313, Loss: 1.0423\n",
            "Epoch 5, Batch 160/313, Loss: 1.1991\n",
            "Epoch 5, Batch 170/313, Loss: 1.5915\n",
            "Epoch 5, Batch 180/313, Loss: 0.8824\n",
            "Epoch 5, Batch 190/313, Loss: 1.2087\n",
            "Epoch 5, Batch 200/313, Loss: 1.3831\n",
            "Epoch 5, Batch 210/313, Loss: 1.3205\n",
            "Epoch 5, Batch 220/313, Loss: 0.9365\n",
            "Epoch 5, Batch 230/313, Loss: 0.8832\n",
            "Epoch 5, Batch 240/313, Loss: 1.0240\n",
            "Epoch 5, Batch 250/313, Loss: 1.1160\n",
            "Epoch 5, Batch 260/313, Loss: 1.4005\n",
            "Epoch 5, Batch 270/313, Loss: 1.1318\n",
            "Epoch 5, Batch 280/313, Loss: 0.8511\n",
            "Epoch 5, Batch 290/313, Loss: 0.8873\n",
            "Epoch 5, Batch 300/313, Loss: 1.2209\n",
            "Epoch 5, Batch 310/313, Loss: 1.1789\n",
            "Epoch 6/15\n",
            "  Train Loss: 1.1127, Train Acc: 0.6256\n",
            "  Val Loss: 1.2285, Val Acc: 0.5843, Val F1: 0.5391\n",
            "Epoch 6, Batch 10/313, Loss: 0.8116\n",
            "Epoch 6, Batch 20/313, Loss: 0.9910\n",
            "Epoch 6, Batch 30/313, Loss: 1.1120\n",
            "Epoch 6, Batch 40/313, Loss: 1.2931\n",
            "Epoch 6, Batch 50/313, Loss: 1.1214\n",
            "Epoch 6, Batch 60/313, Loss: 1.3400\n",
            "Epoch 6, Batch 70/313, Loss: 0.9962\n",
            "Epoch 6, Batch 80/313, Loss: 1.0523\n",
            "Epoch 6, Batch 90/313, Loss: 1.0736\n",
            "Epoch 6, Batch 100/313, Loss: 1.4495\n",
            "Epoch 6, Batch 110/313, Loss: 1.4489\n",
            "Epoch 6, Batch 120/313, Loss: 0.8190\n",
            "Epoch 6, Batch 130/313, Loss: 1.0440\n",
            "Epoch 6, Batch 140/313, Loss: 0.9592\n",
            "Epoch 6, Batch 150/313, Loss: 1.1811\n",
            "Epoch 6, Batch 160/313, Loss: 1.5391\n",
            "Epoch 6, Batch 170/313, Loss: 0.9949\n",
            "Epoch 6, Batch 180/313, Loss: 1.0767\n",
            "Epoch 6, Batch 190/313, Loss: 0.9625\n",
            "Epoch 6, Batch 200/313, Loss: 0.8455\n",
            "Epoch 6, Batch 210/313, Loss: 1.5089\n",
            "Epoch 6, Batch 220/313, Loss: 0.8522\n",
            "Epoch 6, Batch 230/313, Loss: 0.9658\n",
            "Epoch 6, Batch 240/313, Loss: 0.9903\n",
            "Epoch 6, Batch 250/313, Loss: 1.0134\n",
            "Epoch 6, Batch 260/313, Loss: 1.0291\n",
            "Epoch 6, Batch 270/313, Loss: 1.2505\n",
            "Epoch 6, Batch 280/313, Loss: 0.8868\n",
            "Epoch 6, Batch 290/313, Loss: 1.1759\n",
            "Epoch 6, Batch 300/313, Loss: 1.2946\n",
            "Epoch 6, Batch 310/313, Loss: 0.9853\n",
            "Epoch 7/15\n",
            "  Train Loss: 1.0984, Train Acc: 0.6262\n",
            "  Val Loss: 1.2314, Val Acc: 0.5897, Val F1: 0.5433\n",
            "  Best model saved!\n",
            "Epoch 7, Batch 10/313, Loss: 0.9295\n",
            "Epoch 7, Batch 20/313, Loss: 1.2204\n",
            "Epoch 7, Batch 30/313, Loss: 0.9749\n",
            "Epoch 7, Batch 40/313, Loss: 1.4094\n",
            "Epoch 7, Batch 50/313, Loss: 1.1613\n",
            "Epoch 7, Batch 60/313, Loss: 0.9460\n",
            "Epoch 7, Batch 70/313, Loss: 1.0362\n",
            "Epoch 7, Batch 80/313, Loss: 0.8363\n",
            "Epoch 7, Batch 90/313, Loss: 0.8031\n",
            "Epoch 7, Batch 100/313, Loss: 1.0422\n",
            "Epoch 7, Batch 110/313, Loss: 1.0977\n",
            "Epoch 7, Batch 120/313, Loss: 1.1555\n",
            "Epoch 7, Batch 130/313, Loss: 1.0879\n",
            "Epoch 7, Batch 140/313, Loss: 1.3371\n",
            "Epoch 7, Batch 150/313, Loss: 0.8343\n",
            "Epoch 7, Batch 160/313, Loss: 0.8628\n",
            "Epoch 7, Batch 170/313, Loss: 1.1780\n",
            "Epoch 7, Batch 180/313, Loss: 1.0006\n",
            "Epoch 7, Batch 190/313, Loss: 0.7457\n",
            "Epoch 7, Batch 200/313, Loss: 0.9548\n",
            "Epoch 7, Batch 210/313, Loss: 0.9460\n",
            "Epoch 7, Batch 220/313, Loss: 0.9232\n",
            "Epoch 7, Batch 230/313, Loss: 0.9634\n",
            "Epoch 7, Batch 240/313, Loss: 1.0615\n",
            "Epoch 7, Batch 250/313, Loss: 1.0964\n",
            "Epoch 7, Batch 260/313, Loss: 0.9459\n",
            "Epoch 7, Batch 270/313, Loss: 0.9417\n",
            "Epoch 7, Batch 280/313, Loss: 1.1251\n",
            "Epoch 7, Batch 290/313, Loss: 0.9241\n",
            "Epoch 7, Batch 300/313, Loss: 0.8672\n",
            "Epoch 7, Batch 310/313, Loss: 1.2106\n",
            "Epoch 8/15\n",
            "  Train Loss: 1.0747, Train Acc: 0.6334\n",
            "  Val Loss: 1.2297, Val Acc: 0.5897, Val F1: 0.5469\n",
            "Epoch 8, Batch 10/313, Loss: 1.0754\n",
            "Epoch 8, Batch 20/313, Loss: 1.0443\n",
            "Epoch 8, Batch 30/313, Loss: 0.8315\n",
            "Epoch 8, Batch 40/313, Loss: 1.1223\n",
            "Epoch 8, Batch 50/313, Loss: 1.0035\n",
            "Epoch 8, Batch 60/313, Loss: 1.2546\n",
            "Epoch 8, Batch 70/313, Loss: 0.9387\n",
            "Epoch 8, Batch 80/313, Loss: 1.1725\n",
            "Epoch 8, Batch 90/313, Loss: 1.0724\n",
            "Epoch 8, Batch 100/313, Loss: 1.2101\n",
            "Epoch 8, Batch 110/313, Loss: 1.4225\n",
            "Epoch 8, Batch 120/313, Loss: 1.0639\n",
            "Epoch 8, Batch 130/313, Loss: 0.9553\n",
            "Epoch 8, Batch 140/313, Loss: 0.8716\n",
            "Epoch 8, Batch 150/313, Loss: 0.7393\n",
            "Epoch 8, Batch 160/313, Loss: 1.1750\n",
            "Epoch 8, Batch 170/313, Loss: 1.2165\n",
            "Epoch 8, Batch 180/313, Loss: 1.1859\n",
            "Epoch 8, Batch 190/313, Loss: 0.8902\n",
            "Epoch 8, Batch 200/313, Loss: 1.0066\n",
            "Epoch 8, Batch 210/313, Loss: 1.0755\n",
            "Epoch 8, Batch 220/313, Loss: 0.9066\n",
            "Epoch 8, Batch 230/313, Loss: 1.2063\n",
            "Epoch 8, Batch 240/313, Loss: 0.8186\n",
            "Epoch 8, Batch 250/313, Loss: 0.6822\n",
            "Epoch 8, Batch 260/313, Loss: 1.3219\n",
            "Epoch 8, Batch 270/313, Loss: 0.8366\n",
            "Epoch 8, Batch 280/313, Loss: 1.0911\n",
            "Epoch 8, Batch 290/313, Loss: 0.9072\n",
            "Epoch 8, Batch 300/313, Loss: 0.9277\n",
            "Epoch 8, Batch 310/313, Loss: 1.2367\n",
            "Epoch 9/15\n",
            "  Train Loss: 1.0610, Train Acc: 0.6352\n",
            "  Val Loss: 1.2269, Val Acc: 0.5996, Val F1: 0.5556\n",
            "  Best model saved!\n",
            "Epoch 9, Batch 10/313, Loss: 1.0205\n",
            "Epoch 9, Batch 20/313, Loss: 0.8045\n",
            "Epoch 9, Batch 30/313, Loss: 1.0530\n",
            "Epoch 9, Batch 40/313, Loss: 1.1786\n",
            "Epoch 9, Batch 50/313, Loss: 0.9510\n",
            "Epoch 9, Batch 60/313, Loss: 1.2226\n",
            "Epoch 9, Batch 70/313, Loss: 1.0398\n",
            "Epoch 9, Batch 80/313, Loss: 1.0885\n",
            "Epoch 9, Batch 90/313, Loss: 1.3398\n",
            "Epoch 9, Batch 100/313, Loss: 1.1326\n",
            "Epoch 9, Batch 110/313, Loss: 0.9405\n",
            "Epoch 9, Batch 120/313, Loss: 1.1972\n",
            "Epoch 9, Batch 130/313, Loss: 1.1628\n",
            "Epoch 9, Batch 140/313, Loss: 1.0940\n",
            "Epoch 9, Batch 150/313, Loss: 0.6147\n",
            "Epoch 9, Batch 160/313, Loss: 1.0822\n",
            "Epoch 9, Batch 170/313, Loss: 1.1570\n",
            "Epoch 9, Batch 180/313, Loss: 0.7353\n",
            "Epoch 9, Batch 190/313, Loss: 1.5394\n",
            "Epoch 9, Batch 200/313, Loss: 1.0131\n",
            "Epoch 9, Batch 210/313, Loss: 0.9729\n",
            "Epoch 9, Batch 220/313, Loss: 0.9965\n",
            "Epoch 9, Batch 230/313, Loss: 0.8296\n",
            "Epoch 9, Batch 240/313, Loss: 0.6813\n",
            "Epoch 9, Batch 250/313, Loss: 1.0080\n",
            "Epoch 9, Batch 260/313, Loss: 1.1167\n",
            "Epoch 9, Batch 270/313, Loss: 1.5100\n",
            "Epoch 9, Batch 280/313, Loss: 0.8375\n",
            "Epoch 9, Batch 290/313, Loss: 0.9499\n",
            "Epoch 9, Batch 300/313, Loss: 1.0099\n",
            "Epoch 9, Batch 310/313, Loss: 0.8572\n",
            "Epoch 10/15\n",
            "  Train Loss: 1.0417, Train Acc: 0.6441\n",
            "  Val Loss: 1.2232, Val Acc: 0.5888, Val F1: 0.5458\n",
            "Epoch 10, Batch 10/313, Loss: 1.1704\n",
            "Epoch 10, Batch 20/313, Loss: 1.2735\n",
            "Epoch 10, Batch 30/313, Loss: 1.2821\n",
            "Epoch 10, Batch 40/313, Loss: 0.9862\n",
            "Epoch 10, Batch 50/313, Loss: 0.9494\n",
            "Epoch 10, Batch 60/313, Loss: 1.2296\n",
            "Epoch 10, Batch 70/313, Loss: 0.9756\n",
            "Epoch 10, Batch 80/313, Loss: 0.8668\n",
            "Epoch 10, Batch 90/313, Loss: 0.9454\n",
            "Epoch 10, Batch 100/313, Loss: 1.4182\n",
            "Epoch 10, Batch 110/313, Loss: 0.8687\n",
            "Epoch 10, Batch 120/313, Loss: 1.1873\n",
            "Epoch 10, Batch 130/313, Loss: 0.9654\n",
            "Epoch 10, Batch 140/313, Loss: 0.9625\n",
            "Epoch 10, Batch 150/313, Loss: 1.2724\n",
            "Epoch 10, Batch 160/313, Loss: 0.8992\n",
            "Epoch 10, Batch 170/313, Loss: 1.0140\n",
            "Epoch 10, Batch 180/313, Loss: 1.2865\n",
            "Epoch 10, Batch 190/313, Loss: 1.0987\n",
            "Epoch 10, Batch 200/313, Loss: 1.0276\n",
            "Epoch 10, Batch 210/313, Loss: 1.1485\n",
            "Epoch 10, Batch 220/313, Loss: 0.8814\n",
            "Epoch 10, Batch 230/313, Loss: 1.0721\n",
            "Epoch 10, Batch 240/313, Loss: 0.9603\n",
            "Epoch 10, Batch 250/313, Loss: 1.1249\n",
            "Epoch 10, Batch 260/313, Loss: 1.1114\n",
            "Epoch 10, Batch 270/313, Loss: 1.3235\n",
            "Epoch 10, Batch 280/313, Loss: 1.0898\n",
            "Epoch 10, Batch 290/313, Loss: 1.3558\n",
            "Epoch 10, Batch 300/313, Loss: 0.7471\n",
            "Epoch 10, Batch 310/313, Loss: 0.9633\n",
            "Epoch 11/15\n",
            "  Train Loss: 1.0216, Train Acc: 0.6455\n",
            "  Val Loss: 1.2278, Val Acc: 0.5951, Val F1: 0.5557\n",
            "Epoch 11, Batch 10/313, Loss: 1.1074\n",
            "Epoch 11, Batch 20/313, Loss: 1.2808\n",
            "Epoch 11, Batch 30/313, Loss: 0.9955\n",
            "Epoch 11, Batch 40/313, Loss: 0.9293\n",
            "Epoch 11, Batch 50/313, Loss: 0.9174\n",
            "Epoch 11, Batch 60/313, Loss: 0.9917\n",
            "Epoch 11, Batch 70/313, Loss: 1.0804\n",
            "Epoch 11, Batch 80/313, Loss: 0.9396\n",
            "Epoch 11, Batch 90/313, Loss: 0.9827\n",
            "Epoch 11, Batch 100/313, Loss: 0.7452\n",
            "Epoch 11, Batch 110/313, Loss: 0.9712\n",
            "Epoch 11, Batch 120/313, Loss: 1.0716\n",
            "Epoch 11, Batch 130/313, Loss: 1.0119\n",
            "Epoch 11, Batch 140/313, Loss: 0.8573\n",
            "Epoch 11, Batch 150/313, Loss: 0.8929\n",
            "Epoch 11, Batch 160/313, Loss: 0.9548\n",
            "Epoch 11, Batch 170/313, Loss: 0.6784\n",
            "Epoch 11, Batch 180/313, Loss: 0.9758\n",
            "Epoch 11, Batch 190/313, Loss: 1.1552\n",
            "Epoch 11, Batch 200/313, Loss: 1.3178\n",
            "Epoch 11, Batch 210/313, Loss: 0.9887\n",
            "Epoch 11, Batch 220/313, Loss: 0.9352\n",
            "Epoch 11, Batch 230/313, Loss: 1.0909\n",
            "Epoch 11, Batch 240/313, Loss: 1.2592\n",
            "Epoch 11, Batch 250/313, Loss: 1.1397\n",
            "Epoch 11, Batch 260/313, Loss: 0.8292\n",
            "Epoch 11, Batch 270/313, Loss: 0.9522\n",
            "Epoch 11, Batch 280/313, Loss: 1.0833\n",
            "Epoch 11, Batch 290/313, Loss: 1.0167\n",
            "Epoch 11, Batch 300/313, Loss: 1.1000\n",
            "Epoch 11, Batch 310/313, Loss: 0.7979\n",
            "Epoch 12/15\n",
            "  Train Loss: 1.0182, Train Acc: 0.6508\n",
            "  Val Loss: 1.2375, Val Acc: 0.5942, Val F1: 0.5551\n",
            "Epoch 12, Batch 10/313, Loss: 1.0455\n",
            "Epoch 12, Batch 20/313, Loss: 0.8688\n",
            "Epoch 12, Batch 30/313, Loss: 0.9371\n",
            "Epoch 12, Batch 40/313, Loss: 0.8675\n",
            "Epoch 12, Batch 50/313, Loss: 0.8900\n",
            "Epoch 12, Batch 60/313, Loss: 1.1651\n",
            "Epoch 12, Batch 70/313, Loss: 0.9445\n",
            "Epoch 12, Batch 80/313, Loss: 1.1330\n",
            "Epoch 12, Batch 90/313, Loss: 0.7691\n",
            "Epoch 12, Batch 100/313, Loss: 1.0812\n",
            "Epoch 12, Batch 110/313, Loss: 0.8630\n",
            "Epoch 12, Batch 120/313, Loss: 0.9685\n",
            "Epoch 12, Batch 130/313, Loss: 1.0170\n",
            "Epoch 12, Batch 140/313, Loss: 0.9357\n",
            "Epoch 12, Batch 150/313, Loss: 1.0251\n",
            "Epoch 12, Batch 160/313, Loss: 0.9824\n",
            "Epoch 12, Batch 170/313, Loss: 1.2036\n",
            "Epoch 12, Batch 180/313, Loss: 0.7472\n",
            "Epoch 12, Batch 190/313, Loss: 0.9527\n",
            "Epoch 12, Batch 200/313, Loss: 1.1520\n",
            "Epoch 12, Batch 210/313, Loss: 0.9378\n",
            "Epoch 12, Batch 220/313, Loss: 1.3678\n",
            "Epoch 12, Batch 230/313, Loss: 1.0966\n",
            "Epoch 12, Batch 240/313, Loss: 0.8387\n",
            "Epoch 12, Batch 250/313, Loss: 0.6900\n",
            "Epoch 12, Batch 260/313, Loss: 0.7929\n",
            "Epoch 12, Batch 270/313, Loss: 0.8977\n",
            "Epoch 12, Batch 280/313, Loss: 1.2962\n",
            "Epoch 12, Batch 290/313, Loss: 0.7564\n",
            "Epoch 12, Batch 300/313, Loss: 0.9524\n",
            "Epoch 12, Batch 310/313, Loss: 0.8119\n",
            "Epoch 13/15\n",
            "  Train Loss: 1.0125, Train Acc: 0.6556\n",
            "  Val Loss: 1.2320, Val Acc: 0.5969, Val F1: 0.5593\n",
            "Epoch 13, Batch 10/313, Loss: 1.0906\n",
            "Epoch 13, Batch 20/313, Loss: 1.0071\n",
            "Epoch 13, Batch 30/313, Loss: 1.1380\n",
            "Epoch 13, Batch 40/313, Loss: 1.0957\n",
            "Epoch 13, Batch 50/313, Loss: 0.5463\n",
            "Epoch 13, Batch 60/313, Loss: 1.2597\n",
            "Epoch 13, Batch 70/313, Loss: 1.0524\n",
            "Epoch 13, Batch 80/313, Loss: 1.0887\n",
            "Epoch 13, Batch 90/313, Loss: 1.2809\n",
            "Epoch 13, Batch 100/313, Loss: 1.2733\n",
            "Epoch 13, Batch 110/313, Loss: 0.8797\n",
            "Epoch 13, Batch 120/313, Loss: 0.9851\n",
            "Epoch 13, Batch 130/313, Loss: 1.0758\n",
            "Epoch 13, Batch 140/313, Loss: 1.0897\n",
            "Epoch 13, Batch 150/313, Loss: 0.9775\n",
            "Epoch 13, Batch 160/313, Loss: 0.8692\n",
            "Epoch 13, Batch 170/313, Loss: 0.8434\n",
            "Epoch 13, Batch 180/313, Loss: 1.0491\n",
            "Epoch 13, Batch 190/313, Loss: 0.9394\n",
            "Epoch 13, Batch 200/313, Loss: 1.1816\n",
            "Epoch 13, Batch 210/313, Loss: 1.0135\n",
            "Epoch 13, Batch 220/313, Loss: 0.9451\n",
            "Epoch 13, Batch 230/313, Loss: 0.9510\n",
            "Epoch 13, Batch 240/313, Loss: 1.1146\n",
            "Epoch 13, Batch 250/313, Loss: 1.1256\n",
            "Epoch 13, Batch 260/313, Loss: 0.8089\n",
            "Epoch 13, Batch 270/313, Loss: 0.6623\n",
            "Epoch 13, Batch 280/313, Loss: 1.1355\n",
            "Epoch 13, Batch 290/313, Loss: 1.0244\n",
            "Epoch 13, Batch 300/313, Loss: 1.0551\n",
            "Epoch 13, Batch 310/313, Loss: 0.9349\n",
            "Epoch 14/15\n",
            "  Train Loss: 0.9836, Train Acc: 0.6631\n",
            "  Val Loss: 1.2424, Val Acc: 0.5951, Val F1: 0.5562\n",
            "Epoch 14, Batch 10/313, Loss: 1.1130\n",
            "Epoch 14, Batch 20/313, Loss: 0.8644\n",
            "Epoch 14, Batch 30/313, Loss: 1.0667\n",
            "Epoch 14, Batch 40/313, Loss: 0.8722\n",
            "Epoch 14, Batch 50/313, Loss: 0.7969\n",
            "Epoch 14, Batch 60/313, Loss: 0.9399\n",
            "Epoch 14, Batch 70/313, Loss: 1.0647\n",
            "Epoch 14, Batch 80/313, Loss: 1.1230\n",
            "Epoch 14, Batch 90/313, Loss: 0.8188\n",
            "Epoch 14, Batch 100/313, Loss: 0.7082\n",
            "Epoch 14, Batch 110/313, Loss: 1.0497\n",
            "Epoch 14, Batch 120/313, Loss: 0.9363\n",
            "Epoch 14, Batch 130/313, Loss: 0.7821\n",
            "Epoch 14, Batch 140/313, Loss: 0.8471\n",
            "Epoch 14, Batch 150/313, Loss: 0.8911\n",
            "Epoch 14, Batch 160/313, Loss: 0.7610\n",
            "Epoch 14, Batch 170/313, Loss: 0.9627\n",
            "Epoch 14, Batch 180/313, Loss: 0.9165\n",
            "Epoch 14, Batch 190/313, Loss: 0.7529\n",
            "Epoch 14, Batch 200/313, Loss: 0.9768\n",
            "Epoch 14, Batch 210/313, Loss: 0.7815\n",
            "Epoch 14, Batch 220/313, Loss: 1.4802\n",
            "Epoch 14, Batch 230/313, Loss: 0.8537\n",
            "Epoch 14, Batch 240/313, Loss: 0.7998\n",
            "Epoch 14, Batch 250/313, Loss: 0.8273\n",
            "Epoch 14, Batch 260/313, Loss: 1.0182\n",
            "Epoch 14, Batch 270/313, Loss: 0.9619\n",
            "Epoch 14, Batch 280/313, Loss: 1.1046\n",
            "Epoch 14, Batch 290/313, Loss: 1.2030\n",
            "Epoch 14, Batch 300/313, Loss: 0.8950\n",
            "Epoch 14, Batch 310/313, Loss: 1.0609\n",
            "Epoch 15/15\n",
            "  Train Loss: 0.9666, Train Acc: 0.6597\n",
            "  Val Loss: 1.2397, Val Acc: 0.5951, Val F1: 0.5551\n",
            "\n",
            "Loading best model for evaluation...\n",
            "\n",
            "Evaluating on test set...\n",
            "\n",
            "Test Results:\n",
            "  Test Loss: 1.1188\n",
            "  Test Accuracy: 0.6238\n",
            "  Test F1: 0.5864\n",
            "\n",
            "Running comparative modality analysis...\n",
            "\n",
            "Results saved to results/results_cv_grade_20251102_145542.json\n",
            "\n",
            "CV-Grade Architecture Training Complete!\n"
          ]
        }
      ]
    }
  ]
}